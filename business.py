# Business logic related to transcription
import os
import warnings
import time
from pathlib import Path
from datetime import timedelta
from PySide6.QtCore import QThread, Signal

try:
    import whisper
except ImportError:  # pragma: no cover - optional dependency
    whisper = None


class TranscriptionThread(QThread):
    """Thread that performs transcription of multiple files."""

    progress = Signal(int)
    status = Signal(str)
    finished = Signal(str, str)  # file_path, output_path
    error = Signal(str, str)  # file_path, error_message
    current_file = Signal(str)
    overall_progress = Signal(int, int)  # current, total
    benchmark_result = Signal(str, float)  # engine_name, time_per_minute

    def __init__(self, file_paths, model_size, output_format="srt", backend="whisper"):
        super().__init__()
        self.file_paths = file_paths
        self.model_size = model_size
        self.output_format = output_format
        self.backend = backend
        self.is_running = True
        self.total_audio_duration = 0
        self.total_processing_time = 0

    def run(self):
        try:
            self.status.emit("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

            # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ symlinks –∏ pkg_resources
            os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            try:
                import torch
                device_info = "GPU (CUDA)" if torch.cuda.is_available() else "CPU"
            except ImportError:
                device_info = "CPU"

            start_load_time = time.time()

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                if self.backend == "faster-whisper":
                    from faster_whisper import WhisperModel
                    import torch

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                    device = "cuda" if torch.cuda.is_available() else "cpu"

                    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π compute_type –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
                    if device == "cuda":
                        compute_type = "float16"  # –ë—ã—Å—Ç—Ä–µ–µ –Ω–∞ GPU
                    else:
                        compute_type = "int8"  # –ë—ã—Å—Ç—Ä–µ–µ –Ω–∞ CPU

                    model = WhisperModel(
                        self.model_size,
                        device=device,
                        compute_type=compute_type,
                        cpu_threads=os.cpu_count(),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞ CPU
                        num_workers=4,  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    )
                else:
                    if whisper is None:
                        raise RuntimeError(
                            "whisper package is not installed, –≤—ã–±—Ä–∞—Ç—å 'faster-whisper'"
                        )
                    model = whisper.load_model(self.model_size)

            load_time = time.time() - start_load_time
            self.status.emit(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({device_info}) –∑–∞ {load_time:.1f}—Å")

            total_files = len(self.file_paths)

            for index, file_path in enumerate(self.file_paths):
                if not self.is_running:
                    break

                self.current_file.emit(os.path.basename(file_path))
                self.overall_progress.emit(index + 1, total_files)

                try:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ
                    audio_duration = self.get_audio_duration(file_path)

                    self.status.emit("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–∏–µ")
                    self.progress.emit(30)

                    start_time = time.time()

                    if self.backend == "faster-whisper":
                        segments, info = model.transcribe(
                            file_path,
                            language="en",
                            beam_size=1,  # –ë—ã—Å—Ç—Ä–µ–µ —Å beam_size=1
                            vad_filter=True,  # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–∏—à–∏–Ω—ã
                            vad_parameters=dict(
                                min_silence_duration_ms=500,
                            ),
                        )
                        result_segments = [
                            {"start": s.start, "end": s.end, "text": s.text}
                            for s in segments
                        ]
                        # –ü–æ–ª—É—á–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ info –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                        if hasattr(info, 'duration') and info.duration:
                            audio_duration = info.duration
                    else:
                        result = model.transcribe(file_path, language="en", fp16=False)
                        result_segments = result["segments"]
                        # –ü–æ–ª—É—á–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                        if "segments" in result and result["segments"]:
                            last_segment = result["segments"][-1]
                            audio_duration = last_segment["end"]

                    processing_time = time.time() - start_time

                    # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ
                    if audio_duration > 0:
                        self.total_audio_duration += audio_duration
                        self.total_processing_time += processing_time

                    self.progress.emit(80)

                    input_path = Path(file_path)
                    if self.output_format == "txt":
                        content = self.create_txt(result_segments)
                        output_path = input_path.with_suffix(".txt")
                    else:
                        content = self.create_srt(result_segments)
                        output_path = input_path.with_suffix(".srt")

                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(content)

                    self.progress.emit(100)
                    self.finished.emit(file_path, str(output_path))

                except Exception as e:  # noqa: BLE001
                    self.error.emit(file_path, str(e))

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
            if self.total_audio_duration > 0 and self.total_processing_time > 0:
                # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ –º–∏–Ω—É—Ç—É –∞—É–¥–∏–æ
                time_per_minute = (self.total_processing_time / self.total_audio_duration) * 60
                self.benchmark_result.emit(self.backend, time_per_minute)

        except Exception as e:  # noqa: BLE001
            self.error.emit("", f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

    def stop(self):
        self.is_running = False

    def get_audio_duration(self, file_path):
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ffprobe –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            import subprocess
            import json

            cmd = [
                'ffprobe',
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_format',
                file_path
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                data = json.loads(result.stdout)
                duration = float(data['format']['duration'])
                return duration
        except:
            pass

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
        return 0

    def create_srt(self, segments):
        srt_content = ""
        for i, segment in enumerate(segments, 1):
            start_time = self.format_timestamp(segment["start"])
            end_time = self.format_timestamp(segment["end"])
            text = segment["text"].strip()

            srt_content += f"{i}\n"
            srt_content += f"{start_time} --> {end_time}\n"
            srt_content += f"{text}\n\n"

        return srt_content

    def create_txt(self, segments):
        """Return plain text without time codes from segments."""
        return "\n".join(segment["text"].strip() for segment in segments)

    def format_timestamp(self, seconds):
        td = timedelta(seconds=seconds)
        hours = td.seconds // 3600
        minutes = (td.seconds % 3600) // 60
        seconds = td.seconds % 60
        milliseconds = td.microseconds // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"


class BenchmarkThread(QThread):
    """Thread that performs benchmark comparison between whisper and faster-whisper."""

    status = Signal(str)
    progress = Signal(int)
    result = Signal(str)
    error = Signal(str)

    def __init__(self, test_file, model_size="tiny"):
        super().__init__()
        self.test_file = test_file
        self.model_size = model_size

    def run(self):
        try:
            results = []

            # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
            os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
            warnings.filterwarnings("ignore", message="pkg_resources is deprecated")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            try:
                import torch
                device_info = "GPU (CUDA)" if torch.cuda.is_available() else "CPU"
                has_gpu = torch.cuda.is_available()
            except ImportError:
                device_info = "CPU"
                has_gpu = False

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º faster-whisper
            try:
                self.status.emit("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Faster-Whisper...")
                self.progress.emit(25)

                from faster_whisper import WhisperModel
                import torch

                device = "cuda" if torch.cuda.is_available() else "cpu"
                compute_type = "float16" if device == "cuda" else "int8"

                # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
                start_time = time.time()
                model = WhisperModel(
                    self.model_size,
                    device=device,
                    compute_type=compute_type,
                    cpu_threads=os.cpu_count(),
                    num_workers=4,
                )
                load_time = time.time() - start_time

                # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –≤—Å–µ–≥–¥–∞ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
                self.status.emit("–ü—Ä–æ–≥—Ä–µ–≤ Faster-Whisper...")
                warmup_segments, _ = model.transcribe(
                    self.test_file,
                    language="en",
                    beam_size=1,
                    vad_filter=True,
                )
                _ = list(warmup_segments)  # –ü–æ—Ç—Ä–µ–±–ª—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä

                # –†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç
                self.status.emit("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Faster-Whisper (–æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫)...")
                start_time = time.time()
                segments, info = model.transcribe(
                    self.test_file,
                    language="en",
                    beam_size=1,
                    vad_filter=True,
                )
                # –ü–æ—Ç—Ä–µ–±–ª—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
                _ = list(segments)
                transcribe_time = time.time() - start_time

                audio_duration = info.duration if hasattr(info, 'duration') else 60
                time_per_minute = (transcribe_time / audio_duration) * 60

                results.append(f"üöÄ Faster-Whisper ({device_info}):")
                results.append(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {load_time:.2f}—Å")
                results.append(f"   –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {transcribe_time:.2f}—Å")
                results.append(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {time_per_minute:.2f}—Å –Ω–∞ –º–∏–Ω—É—Ç—É –∞—É–¥–∏–æ")
                results.append(f"   –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {audio_duration / transcribe_time:.1f}x")

                del model  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å

            except Exception as e:
                results.append(f"‚ùå Faster-Whisper: {str(e)}")

            self.progress.emit(50)

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π whisper
            if whisper is not None:
                try:
                    self.status.emit("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ OpenAI Whisper...")
                    self.progress.emit(75)

                    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
                    start_time = time.time()
                    model = whisper.load_model(self.model_size)
                    load_time = time.time() - start_time

                    # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
                    self.status.emit("–ü—Ä–æ–≥—Ä–µ–≤ OpenAI Whisper...")
                    _ = model.transcribe(self.test_file, language="en", fp16=False)

                    # –†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç
                    self.status.emit("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ OpenAI Whisper (–æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫)...")
                    start_time = time.time()
                    result = model.transcribe(self.test_file, language="en", fp16=False)
                    transcribe_time = time.time() - start_time

                    if result["segments"]:
                        audio_duration = result["segments"][-1]["end"]
                    else:
                        audio_duration = 60  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

                    time_per_minute = (transcribe_time / audio_duration) * 60

                    results.append(f"\nüê¢ OpenAI Whisper ({device_info}):")
                    results.append(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {load_time:.2f}—Å")
                    results.append(f"   –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {transcribe_time:.2f}—Å")
                    results.append(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {time_per_minute:.2f}—Å –Ω–∞ –º–∏–Ω—É—Ç—É –∞—É–¥–∏–æ")
                    results.append(f"   –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {audio_duration / transcribe_time:.1f}x")

                    del model  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å

                except Exception as e:
                    results.append(f"\n‚ùå OpenAI Whisper: {str(e)}")
            else:
                results.append(f"\n‚ùå OpenAI Whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

            self.progress.emit(100)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
            summary = f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ (–º–æ–¥–µ–ª—å: {self.model_size})\n"
            summary += f"üìÅ –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {os.path.basename(self.test_file)}\n"
            summary += f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_info}\n"
            summary += "‚îÄ" * 50 + "\n"
            summary += "\n".join(results)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            summary += "\n\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n"
            if not has_gpu:
                summary += "‚Ä¢ –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 4-5 —Ä–∞–∑ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU (NVIDIA —Å CUDA)\n"
            summary += "‚Ä¢ Faster-Whisper —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö\n"
            summary += "‚Ä¢ –ú–æ–¥–µ–ª—å 'base' –¥–∞–µ—Ç –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞\n"
            if device_info == "CPU":
                summary += "‚Ä¢ –ù–∞ CPU —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –¥–≤–∏–∂–∫–∞–º–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞\n"

            self.result.emit(summary)

        except Exception as e:
            self.error.emit(f"–û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {str(e)}")